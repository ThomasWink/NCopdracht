% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
% A lot of package loading
\usepackage[pdftex]{graphicx}
\usepackage{geometry}
\usepackage[cmex10]{amsmath}
\usepackage{array, algpseudocode}
\usepackage{amsmath, amssymb, amsfonts, parskip, graphicx, verbatim}
\usepackage{url, hyperref}
\usepackage{bm, rotating, adjustbox, latexsym}
\usepackage{tabularx, booktabs}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage{float, setspace, mdframed}
\usepackage{color, contour, placeins, subfig, cite}
\usepackage[mathscr]{euscript}
\usepackage[osf]{mathpazo}
\usepackage{pgf, tikz, microtype, algorithm}
\usetikzlibrary{shapes,backgrounds,calc,arrows}
\usepackage{xcolor, colortbl, dsfont}


% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\renewcommand\UrlFont{\color{blue}\rmfamily}

\graphicspath{{figures/}}

\begin{document}
%
\title{Shark Smell Optimization}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Jochem Ram (S2040328), Jerry Schonenberg (S2041022), Wouter van Tol (S2041340), Thomas Wink (S1705148)}
%
\authorrunning{J. Ram, J. Schonenberg, et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Leiden Institute of Advanced Computer Science, The Netherlands}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
In this paper a pseudocode is presented for the Shark Smell Optimization algorithm.
\end{abstract}





\section{Introduction}
For as long as humans exist, we have looked at nature for inspiration. Most inventions humans have made come from investigating natural phenomena and trying to reproduce what we see. This is still the case nowadays, where we think we know almost everything. It has proven useful to examine the way processes happen in nature and to try to implement this in problems we have ourselves. One example is to look at the way a shark, one of the best hunters in the sea, finds a pray in an efficient way. This can be used in an optimization problem to quickly find the best solution to a complex problem in a high dimensional solution space.\\
In this paper we present the findings of this shark-smell-optimization (SSO) algorithm. This will be done using IOHprofiler by comparing the performance of the algorithm for different objective functions.\\
The algorithm was originally introduced here \cite{abedinia2014shark}.

\section{Algorithm Description} \label{sec:description}
This algorithm uses a "shark", an individual, to find a "wounded fish", the maximal value of the objective function, by following the "blood particles", the gradient of the objective function, and performing "circular searches", local searches around the individual's position at each stage. This is repeated for each individual until stage $k_{max}$ is reached. The number of individuals and stages can be decided by the user. \\
A single iteration of the algorithm will first contain the calculation of the movement of the individuals in each dimension of the search space. Then a new position is calculated for every position. From this position a random local search will be done, where the best point from this search will be compared with the position from the the first search. The best one of these will be the new starting position for this individual for the next iteration.

%Give a general overview of the working principles of your algorithm. Make sure to always put quotation marks around animal names, and try to use strict formulations. For example, you can introduce your algorithm by referring to 'bats', but afterwards you should refer to them as individuals or search-points. 
%Make sure this description is complete. It should explain all core concepts used in your algorithm, such that one should not need to refer back to the original paper to get the global picture of the working principals.

\subsection{Comparison with standard algorithms}
The SSO algorithm can be best compared to simulated annealing. This algorithm is based on the concept of heat treatment in metals, in which a hot metal is slowly heated to let the atoms slowly settle to minimal energy states. This is used in optimization by perturbing each point randomly and choosing whether to move to that point or not. If the objective function in that point is higher, it will always move. If the objective function is lower, the chance of movement is calculated with the difference in the objective function values.\\
This algorithm matches the closest to SSO because, unlike almost all natural optimization algorithms, these two algorithms are not based on mutation or swarm behavior. They are based on the searches of individual candidate solutions without any communication between the individuals. $\boldface{Also, both algorithms increase their stochastic search value.}$ SSO does this by choosing random values around the solution candidate and evaluating all of them at once and simulated annealing does this by choosing a random value around the solution candidate and evaluating immediately. For simulated annealing this leads to a higher chance of reaching a local minimum, but this is mitigated by offering the chance of a restart where an individual is placed back to a previously found significantly better solution candidate.\\
However, where the SSO uses the gradient to find the movement direction of the search point, simulated annealing uses difference in objective function values in two points. Also, SSO can not move to a position with a lower objective function value while for simulated annealing this is one of it's strengths.\\
%Based on your description of the algorithm, which of the standard algorithms we discussed in the course is the closest match to your algorithm? Describe the matching principles and where they diverge. Try to give some insight into what these differences would contribute to the performance? In which circumstances?

% \subsection{(Optional) Usage of algorithms}

%If you have trouble dividing the work equally, you could have one team member take a look at where your algorithm has been used / cited in the past. This is optional, but could be nice to include for completeness. 

\section{Pseudo-code}
\label{secPseu}
We followed the following notation convention. All user assigned variables are mentioned in Appendix A.
\subsection{Variable explanation}
\begin{itemize}
    \item $N$: The number of individuals in the set/array.
    \item $M$: The dimensionality of the search space.
    \item $k_{max}$: Maximal amount of stages.
    \item $O$: Number of rotational candidate solutions.
    \item $n$, $m$, $k$ and $o$: Administrative array counters for $N$, $M$, $k_{max}$ and $O$ respectively.
    \item $X_n^k$: The solution candidate $\mathds{R}^M$ of an individual at a stage.
    \item $V_n^k$: The calculated movement in $\mathds{R}^M$ for an individual at a stage.
    \item $v_{n,m}^k$: The calculated movement in a direction for an individual at a stage.
    \item $Y_n^k$: The provisional solution candidate in $\mathds{R}^M$ after movement of an individual at a stage.
    \item $Z_n^{k}$: Set of O provisional solution candidates in $\mathds{R}^M$ of an individual at a stage after local search.
    \item $x_m$: A variable in the search space.
    \item $x_m^{\text{min}}$, $x_m^{\text{max}}$: The minimal and maximal values in the search space for a variable.
    \item $f(\mathbf{x})$: Objective function value of $\mathbf{x}$ ($f: \mathds{R}^M \rightarrow \mathds{R})$.
    \item $\leftarrow$: Assignment operator.
    \item $\boldsymbol{\alpha}$: User assigned vector in $\mathds{R}^{k_{max}}$ between 0 and 1. Represents the inertia coefficient for each stage.
    \item $\boldsymbol{\beta}$: User assigned vector in $\mathds{R}^{k_{max}}$ between 0 and 1. Represents the velocity limiter ratio for each stage.
    \item $\boldsymbol{\eta}$: User assigned vector in $\mathds{R}^{k_{max}}$ between 0 and 1. Limits the gradient of the objective function for each stage.
    \item $ r1 $ and $r2 $: Random uniform value between $ 0 $ and $ 1$ to give the algorithm more stochastic search value.
    \item $ R3 $: Vector in $\mathds{R}^M$ with random uniform values between $ -1 $ and $ 1$ to give the algorithm more stochastic search value.
    \item $\arg \max() $: Takes the variable with the highest objective function value.
    \item $\arg \max_{a=1}^A()$: Iterates over a and takes the variable with the highest objective function value.
    \item Further notation convention is based on mathematics.
\end{itemize}

\subsection{Used functions}

1) $
v_{n,m}^k \leftarrow \eta _k \cdot r1 \cdot  \left. \frac{\partial f(x)}{\partial x_m} \right \rvert_{X_{n}^k} + \alpha_k \cdot r2 \cdot v_{n,m}^{k-1}
$ \\ \\
This function is the calculation of movement in dimension m for individual n in stage k. In the first term of the calculation we take the partial derivative of the objective function to $x_m$, and then multiply it with a velocity limiter constant and a random value for extra stochastic search value. In the second term we take the movement from the last stage in this dimension for this individual and multiply it with a random value and an inertia constant. This function is used in line \ref{eq:1}. \\ 

2) $
\left \lvert v_{n,m}^k \right \rvert  > \left \lvert \beta_k \cdot v_{n,m}^{k-1} \right \rvert
$ \\ \\
This line limits the acceleration of the movement of an individual in dimension m. This is done by comparing the new velocity to the velocity in the previous stage multiplied with a velocity limiter constant. This function is used in line \ref{eq:2}. \\ 

3) $
Y_n^{k+1} \leftarrow X_n^k + V_n^k
$ \\ \\
Assignment of provisional new location based on previous location and movement. This function is used in line \ref{eq:3}. \\ 

4) $
Z_n^{k+1, o} \leftarrow Y_n^{k+1} + R3 \cdot V_n^{k}
$ \\ \\
This function finds $O$ new solution candidates in a local search in the vicinity of $Y_n^{k+1}$. This is done with the provisional new position Y, and the velocity of this stage multiplied with a random vector R3. This function is used in line \ref{eq:4}.\\

5) $
X_n^{k+1} \leftarrow \arg \max (\arg \max_{o=1}^O(f(Z_n^{k+1, o})),f(Y_n^{k+1}))
$ \\ \\
This function finds the best new location by comparing the provisional new location with the solution candidates from the local search, and testing their objective function value. This function is used in line \ref{eq:5}.


%Modify the pseudo-code given in Alg.~\ref{Alg:PSO}. \textbf{Do not deviate from the format used here!} Aim to be as precise as possible, and always use mathematical notation instead of referring to 'bats', 'chickens' etc. Please follow the following notation convention:
%\begin{itemize}
%    \item $n$: The dimensionality of the search space
%    \item $\mathbf{x}=(x_1,x_2,\dots,x_n)$: A solution candidate from $\mathds{R}^n$
%    \item $\mathbf{x}_i$: Solution candidate $i$ in the set/array
%    \item $f(\mathbf{x}_i)$: Objective function value of $\mathbf{x}_i$ ($f: \mathds{R}^n \rightarrow \mathds{R})$
%    \item $M$: Number of individuals in set/array
%    \item $\leftarrow$: Assignment operator
%    \item $\bm{\mathcal{U}}(\mathbf{x}^{\text{min}},\mathbf{x}^{\text{max}} )$: Vector sampled uniformly at random. Here it is 'U' for uniform. For other distributions, use for example $\bm{\mathcal{N}}(0,1)$ for a single number sampled according to the \textit{normal} distribution with mean $0$ and variance $1$.
%\end{itemize}
%If you need to use any other notation, please be consistent and clearly define your added notation. In case of doubt, feel free to ask questions on the blackboard forum. 

%Make sure to define any variables / functions you use. This can be done in the pseudocode itself or in a separate (sub)section of text. Use numbered equations to easily refer back to complicated formulas within the pseudocode. 
\newpage
\subsection{Pseudocode}
\vspace{-4mm} 
\begin{algorithm}[!h]
\begin{algorithmic}[1]
    \State{$N \leftarrow User \quad assigned$} \Comment{Initialize} 
    \State{$ k_{max} \leftarrow User \quad assigned $}
    \State{$ k \leftarrow 1$ }
    \State{$ O \leftarrow User \quad assigned $ }
    \State{$ \boldsymbol{\alpha} \leftarrow User \quad assigned $}
    \State{$ \boldsymbol\beta \leftarrow User \quad assigned $}
    \State{$ \boldsymbol\eta \leftarrow User \quad assigned $}
	\For{$n = 1 \rightarrow n $}
	    \For{$m = 1\rightarrow m$}
	        \State{$x_{n,m}^1 \leftarrow \bm{\mathcal{U}}(x_m^{\text{min}},x_m^{\text{max}})$}
	    \EndFor
	\EndFor
	\While{$k\leq k_{max}$}
    	\For{$n = 1 \rightarrow N$}  \Comment{Calculation of movement per individual} 
    	    \For{$m = 1\rightarrow M$}
    	        \State{$v_{n,m}^k \leftarrow \eta _k \cdot r1 \cdot  \left. \frac{\partial f(x)}{\partial x_m} \right \rvert_{X_{n}^k} + \alpha_k \cdot r2 \cdot v_{n,m}^{k-1}$}\label{eq:1}
    	        \If{$\left \lvert v_{n,m}^k \right \rvert  > \left \lvert \beta_k \cdot v_{n,m}^{k-1} \right \rvert $}\label{eq:2}
    	            \State{$\left \lvert v_{n,m}^k \right \rvert \leftarrow \left \lvert \beta_k \cdot v_{n,m}^{k-1} \right \rvert$}
    	       \EndIf
    	    \EndFor
    	\EndFor
    	\For{$n = 1 \rightarrow N$}  \Comment{Provisional assignment of new position per individual} 
    	    \State{$Y_n^{k+1} \leftarrow X_n^k + V_n^k$}\label{eq:3}
    	\EndFor
    	\For{$n = 1 \rightarrow N$} \Comment{Random local search from provisional position per individual} 
    	    \For{$o = 1 \rightarrow O$}
    	        \State{$Z_n^{k+1, o} \leftarrow Y_n^{k+1} + R3 \cdot V_n^{k}$} \Comment{Deviation \label{dev}}\label{eq:4}
    	    \EndFor
    	\EndFor
    	\For{$n = 1 \rightarrow N$} \Comment{Definitive assignment of new position per individual} 
    	   % \For{$o = 1 \rightarrow O$}
    	        \State{$X_n^{k+1} \leftarrow $ $\max (\max_{o=1}^O(f(Z_n^{k+1, o})),f(Y_n^{k+1}))$}\label{eq:5}
    	   % \EndFor
    	   % \State{$X_n^{k+1} \leftarrow $ max($f(X_n^{k+1}), f(Y_n^{k+1})$)}
    	\EndFor
    	\State{$k \leftarrow k+1$} \Comment{Administration}
    \EndWhile
 \end{algorithmic}
\caption{Shark Smell Optimization}
\label{Alg:SSO}
\end{algorithm}
\vspace{-2mm}


\newpage
\subsection{Assumptions}
For this algorithm we did assume that the objective function should be maximized.\\
In our pseudocode, on line \ref{dev}, we deviated from the original paper. In the original paper this line was: \\
$Z_i^{k+1, m} \leftarrow Y_i^{k+1} + R3 \cdot Y_i^{k+1}$ \\
We changed the second $ Y $ to a V, as we think that the searchspace for the rotational movement should be related to the velocity, and not to the current location.\\
%State all assumptions which are being made in your pseudocode. This will always include minimization/maximization, but can also contain perceived vagueness in the original paper, parts of the original algorithm which are ambiguous, \dots

\section{Experiments}

%Explain what experiments you perform. Mention IOHprofiler (experimenter and analyzer)~\cite{IOHprofiler}, COCO/BBOB~\cite{COCO} and any other tools you used, and reference them. 
%Experiments should be performed on the BBOB-suite, functions 1-24, instances 1-5, at least for dimensions 5 and 20, at least 3 runs per instance.

initial velocity is neglected

in de paper:
alpha is 0.1
beta is 4
eta = 0.9

%Provide a table with the parameter values you used during the experiment, as well as the ones set in the original paper (if they are not the same, clearly state why this is the case). 

\subsection{Original experiments}
wat?
huh functions?
algorithms compared to: Adaptive PSO, modified PSO, improved PSO and enhanced PSO en een heleboel andere (de 32 meest gebruikte algoritmes van de ??? richting
Algoritme was vrijwel overal de beste of even goed als de beste andere algoritmes
%Write a very short summary of the experiments performed in the original paper. Include which functions were used and which algorithms it was compared to (and briefly mention where it was performing well).

\section{Results}

%Generate your figures using the IOHanalyzer. You are required to create at least an ECDF-plot (across all functions / dimensions) comparing your algorithm to the BIPOP-CMA-ES~\cite{COCOperformace}. For this comparison you can use the data at \url{http://coco.gforge.inria.fr/data-archive/bbob/2009/BIPOP-CMA-ES_hansen_noiseless.tgz}.

%For the remaining figures, you should judge which are most important by yourself. Attempt to highlight functions on which your algorithm performs particularly good / bad, and try to give some insight into why this happens.

%\textit{Tip: Modify the targets in the ECDF-plot to have 1e-8 as the minimal precision. If you do this, please submit the used targets with this assignment.}

\section{Conclusion}
a:
easy to implement, but with difficulties in finding the right variables to use as there were some inconsistencies in the original paper with variable names. \\
b:
quite original as nothing really looked like it and there were some new ideas, but it wasn't revolutionary since the concept was quite simple and didn't really negate problems with local minimums.\\

%Write a short conclusion summarizing the most important findings of this assignment:
%\begin{itemize}
%    \item How easy or difficult was it to reproduce the algorithm from the paper?
%    \item How original do you consider this algorithm to be compared to those studied in the course?
%    \item Does the algorithm perform similarly to the performance claims from the original paper?
%    \item How does the performance of the algorithm compare to the BIPOP-CMA-ES?
%\end{itemize}



\bibliographystyle{splncs04}
\bibliography{bibliography.bib}

\appendix
\section{Table of user defined constants}
\label{AppA}
\begin{table}[]
\begin{tabular}{|l|l|l|ll}
\cline{1-3}
Variable & Minimum value & Maximum value &  &  \\  \cline{1-3}
$M$  & 1   & undefined   &  &  \\  \cline{1-3}
$N$ & 1 & undefined & & \\ \cline{1-3}
$ k_{max} $ & 1 & undefined &  &  \\  \cline{1-3}
$ O $   & 0    & undefined &  &   \\  \cline{1-3}
$ \boldsymbol{\alpha}_k $ &  0 & 1 &  &  \\  \cline{1-3}
$ \boldsymbol{\beta}_k $ & 0 & undefined &  &  \\  \cline{1-3}
$ \boldsymbol{\eta}_k $ & 0 & 1 &  & \\  \cline{1-3}
\end{tabular}
\end{table}


\section{Supplementary materials}
Next to the report, you are also required to submit the following materials:
\begin{itemize}
    \item The C++ code of your algorithm. Use sufficient comments, modular coding style and clear variable names. This should only be one file which can directly be plugged into the IOHexperimenter (name the file \textit{GroupXX.cpp}). Make sure this file compiles and runs without errors on linux (the machines in rooms 302-306).\\
    \textit{Tip: You can sse \url{https://git.liacs.nl} to host and share code with your teammates. You can log in using your ULCN username and password.}
    \item The runtime data generated by your code. This should be a single zip-file, containing only the data from your algorithm (set algorithm name property to \textit{GroupXX-Animal-Name}).
    \item Per team member: In an email (to \href{mailto:d.l.vermetten@liacs.leidenuniv.nl}{\email{d.l.vermetten@liacs.leidenuniv.nl}}, subject line should include "[NaCo Assignment Review]" + your group number): a grade for your teammates based on their contribution, with some motivation. 
    \item This report in pdf format + the files used to generate it (tex, bib, figures). 
\end{itemize}


\end{document}
